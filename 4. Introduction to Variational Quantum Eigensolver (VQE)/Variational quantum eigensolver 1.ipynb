{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6581d769-6f75-4b26-90e3-39c28f22a74c",
      "metadata": {},
      "source": [
        "# The variational quantum eigensolver (VQE)\n",
        "\n",
        "This lesson will introduce the variational quantum eigensolver, explain its importance as a foundational algorithm in quantum computing, and also explore its strengths and weaknesses. VQE by itself, without augmenting methods, is not likely to be sufficient for modern utility scale quantum computations. It is nevertheless important as an archetypal classical-quantum hybrid method, an it is an important foundation upon which many more advanced algorithms are built."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21115450-3f01-4e19-860d-48e31f401b1f",
      "metadata": {},
      "source": [
        "## 1. What is VQE?\n",
        "\n",
        "The variational quantum eigensolver is an algorithm that uses classical and quantum computing in conjunction to accomplish a task. There are 4 main components of a VQE calculation:\n",
        "\n",
        "*   **An operator**: Often a Hamiltonian, which we’ll call $H$, that describes a property of your system that you wish to optimize. Another way of saying this is that you are seeking the eigenvector of this operator that corresponds to the minimum eigenvalue. We often call that eigenvector the “ground state”.\n",
        "*   **An “ansatz”** (a German word meaning “approach”): this is a quantum circuit that prepares a quantum state approximating the eigenvector you’re seeking. Really the ansatz is a family of quantum circuits, because some of the gates in the ansatz are parametrized, that is, they are fed a parameter which we can vary. This family of quantum circuits can prepare a family of quantum states approximating the ground state.\n",
        "*   **An estimator**: a means of estimating the expectation value of the operator $H$  over the current variational quantum state. Sometimes what we really care about is simply this expectation value, which we call a cost function. Sometimes, we care about a more complicated function that can still be written starting from one or more expectation values.\n",
        "*   **A classical optimizer**: an algorithm that varies parameters to try to minimize the cost function.\n",
        "\n",
        "Let's look at each of these components in more depth."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dbd605e-a440-446b-bc05-e2e301796bf2",
      "metadata": {},
      "source": [
        "### 1.1 The operator (Hamiltonian)\n",
        "\n",
        "At the core of a VQE problem is an operator that describes a system of interest. We will assume here that the lowest eigenvalue and the corresponding eigenvector of this operator are useful for some scientific or business purpose. Examples might include a chemical Hamiltonian describing a molecule, such that the lowest eigenvalue of the operator corresponds to the ground state energy of the molecule, and the corresponding eigenstate describes the geometry or electron configuration of the molecule. Or the operator could describe a cost of a certain process to be optimized, and the eigenstates could correspond to routes or practices. In some fields, like physics, a \"Hamiltonian\" almost always refers to an operator describing the energy of a physical system. But in quantum computing, it is common to see quantum operators that describe a business or logistical problem also referred to as a \"Hamiltonian\". We will adopt that convention here.\n",
        "\n",
        "Mapping a physical or optimization problem to qubits is typically a non-trivial task, but those details are not the focus of this course. A general discussion of mapping a problem to a quantum operator can be found in [Quantum computing in practice](/learning/courses/quantum-computing-in-practice). A more detailed look at the mapping of chemistry problems into quantum operators can be found in [Quantum Chemistry with VQE](/learning/courses/quantum-chem-with-vqe)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f66a1f-93cb-47ee-81c3-6fbf62392003",
      "metadata": {},
      "source": [
        "### 1.3 Estimator\n",
        "\n",
        "We need a way to estimate the expectation value of our Hamiltonian in a particular variational state $|\\psi(\\vec{\\theta})\\rangle$. If we could directly measure the entire operator $H$, this would be as simple as making many (say $N$) measurements and averaging the measured values:\n",
        "\n",
        "$$\n",
        "\\langle \\psi(\\vec{\\theta})|H|\\psi(\\vec{\\theta})\\rangle _N \\approx \\frac{1}{N}\\sum_{j=1}^N {E_j}\n",
        "$$\n",
        "\n",
        "Here, the $\\approx$ symbol reminds us that this expectation value would only be precisely correct in the limit as $N\\rightarrow \\infty$. But with thousands of measurements being made on a circuit, the sampling error of the expectation value is fairly low. There are other considerations such as noise that become an issue for very precise calculations.\n",
        "\n",
        "However, it is generally not possible to measure $H$ all at once. $H$ may contain multiple non-commuting Pauli X, Y, and Z operators. So the Hamiltonian must be broken up into groups of operators that can be simultaneously measured, and each such group must be estimated separately, and the results combined to obtain an expectation value. We will revisit this in greater detail in the next lesson, when we discuss the scaling of classical and quantum approaches. This complexity in measurement is one reason we need highly efficient code for carrying out such estimation. In this lesson and beyond, we will use the Qiskit Runtime primitive Estimator for this purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b68d38e-80ad-4e02-815f-576806ec3e1c",
      "metadata": {},
      "source": [
        "### 1.4 Classical optimizers\n",
        "\n",
        "A classical optimizer is any classical algorithm designed to find extrema of a target function (typically a minimum). They search through the space of possible parameters looking for a set that minimizes some function of interest. They can be broadly categorized into gradient-based methods, which utilize gradient information, and gradient-free methods, which operate as black-box optimizers. The choice of classical optimizer can significantly impact an algorithm's performance, especially in the presence of noise in quantum hardware. Popular optimizers in this field include Adam, AMSGrad, and SPSA, which have shown promising results in noisy environments. More traditional optimizers include COBYLA and SLSQP.\n",
        "\n",
        "A common workflow (demonstrated in Section 3.3) is to use one of these algorithms as the method inside a minimizer like scipy's `minimize` function. This takes as its arguments:\n",
        "\n",
        "*   Some function to be minimized. This is often the energy expectation value. But these are generally referred to as \"cost functions\".\n",
        "*   A set of parameters from which to begin the search. Often called $x_0$ or $\\theta_0$.\n",
        "*   Arguments, including arguments of the cost function. In quantum computing with Qiskit, these arguments will include the ansatz, the Hamiltonian, and the estimator, which is discussed more in the next subsection.\n",
        "*   A 'method' of minimization. This refers to the specific algorithm used to search the parameter space. This is where we would specify, for example, COBYLA or SLSQP.\n",
        "*   Options. The options available may differ by method. But an example which practically all methods would include is the maximum number of iterations of the optimizer before ending the search: 'maxiter'.\n",
        "\n",
        "At each iterative step, the expectation value of the Hamiltonian is estimated by making many measurements. This estimated energy is returned by the cost function, and the minimizer updates the information it has about the energy landscape. Exactly what the optimizer does to choose the next step varies from method to method. Some use gradients and select the direction of steepest descent. Others may take noise into account and may require that the cost decrease by a large margin before accepting that the true energy decreases along that direction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bbcff76-ea91-4fb6-acca-35e9e6675ed1",
      "metadata": {},
      "source": [
        "### 1.5 The variational principle\n",
        "\n",
        "In this context the variational principle is very important; it states that no variational wave function can yield an energy (or cost) expectation value lower than that yielded by the ground state wave function. Mathematically,\n",
        "\n",
        "$$\n",
        "E_\\text{var}=\\langle \\psi_\\text{var}|H|\\psi_\\text{var}\\rangle \\geq E_\\text{min}=\\langle \\psi_\\text{0}|H|\\psi_\\text{0}\\rangle\n",
        "$$\n",
        "\n",
        "This is easy to verify if we note that the set of all eigenstates $\\{|\\psi_0\\rangle, |\\psi_1\\rangle, |\\psi_2\\rangle, ...|\\psi_n \\rangle\\}$ of $H$ form a complete basis for the Hilbert space. In other words, any state and in particular $|\\psi_\\text{var}\\rangle$ can be written as a weighted (normalized) sum of these eigenstates of $H$:\n",
        "\n",
        "$$\n",
        "|\\psi_\\text{var}\\rangle=\\sum_{i=0}^n c_i |\\psi_i\\rangle\n",
        "$$\n",
        "\n",
        "where $c_i$ are constants to be determined, and $\\sum_{i=0} |c_i|^2 = 1$. We leave this as an exercise to the reader. But note the implication: the variational state that produces the lowest-energy expectation value *is* the best estimate of the true ground state."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d14d6df-7320-4f57-bc62-fbee4041957b",
      "metadata": {},
      "source": [
        "## 2. Comparison with classical workflow\n",
        "\n",
        "Let’s say we are interested in a matrix with N rows and N columns. Suppose your matrix is so large that exact diagonalization is not an option. Suppose further that you know enough about your problem that you can make some guesses about the overall structure of the target eigenstate, and you want to probe states similar to your initial guess to see if your cost/energy can be lowered further. This is a variational approach, and it is one method that is used when exact diagonalization is not an option.\n",
        "\n",
        "### 2.1 Classical workflow\n",
        "\n",
        "Using a classical computer, this would work as follows:\n",
        "\n",
        "*   Make a guess state, with some parameters $\\vec{\\theta}_i$ that you will vary: $|\\psi(\\vec{\\theta}_i)\\rangle$. Although this initial guess could be random, that is not advisable. We want to use knowledge of the problem at hand to tailor our guess as much as possible.\n",
        "*   Calculate the expectation value of the operator with the system in that state: $\\langle\\psi(\\vec{\\theta}_i)|H|\\psi(\\vec{\\theta}_i)\\rangle$\n",
        "*   Alter the variational parameters and repeat: $\\vec{\\theta}_i\\rightarrow \\vec{\\theta}_{i+1}$.\n",
        "*   Use accumulated information about the landscape of possible states in your variational subspace to make better and better guesses and approach the target state. The variational principle guarantees that our variational state cannot yield an eigenvalue lower than that of the target ground state. So the lower the expectation value the better our approximation of the ground state:\n",
        "\n",
        "$$\n",
        "\\min_{\\vec{\\theta}} \\{ E_{\\text{var},i} = \\langle\\psi(\\vec{\\theta_i})|H|\\psi(\\vec{\\theta_i})\\rangle \\} \\geq E_0\n",
        "$$\n",
        "\n",
        "Let us examine the difficulty of each step in this approach. Setting or updating parameters is computationally easy; the difficulty there is in selecting useful, physically motivated initial parameters. Using accumulated information from prior iterations to update parameters in such a way that you approach the ground state is a non-trivial. But classical optimization algorithms exist that do this quite efficiently. This classical optimization is only expensive because it may require many iterations; in the worst case, the number of iterations may scale exponentially with N. The most computationally expensive single step is almost certainly calculating the expectation value of your matrix using a given state $|\\psi(\\vec{\\theta_i})\\rangle$: $\\langle\\psi(\\vec{\\theta_i})|H|\\psi(\\vec{\\theta_i})\\rangle.$\n",
        "\n",
        "The $N\\times N$ matrix must act on the $N$-element vector, which corresponds to: $O(N^2)$ multiplication operations in the worst case. This must be done at each iteration of parameters. For extremely large matrices, this has high computational cost.\n",
        "\n",
        "### 2.2 Quantum workflow and commuting Pauli groups\n",
        "\n",
        "Now imagine relegating this portion of the calculation to a quantum computer. Instead of calculating this expectation value, you estimate it by preparing the state $|\\psi(\\vec{\\theta_i})\\rangle$ on the quantum computer using your variational ansatz, and then making measurements.\n",
        "\n",
        "That may sound easier than it is. $H$ is generally not easy to measure. For example it could be made up of many non-commuting Pauli X, Y, and Z operators. But $H$ **can** be written as a linear combination of terms, $h_\\alpha$, each of which is easily measurable (for example, Pauli operators or groups of qubit-wise commuting Pauli operators).\n",
        "The expectation value of $H$ over some state $|\\Psi\\rangle$ is the weighted sum of expectation values of the constituent  terms $h_\\alpha$. This expression holds for any state $|\\Psi⟩$, but we will specifically be using this with our variational states $|\\psi(\\theta_i)\\rangle$.\n",
        "\n",
        "$$\n",
        "H = \\sum_{\\alpha = 1}^T{c_\\alpha h_\\alpha}\n",
        "$$\n",
        "\n",
        "where $h_\\alpha$ is a Pauli string like `IZZX…XIYX`, or several such strings that commute with each other. So a description of the expectation value that more closely matches the realities of measurement on quantum computers is\n",
        "\n",
        "$$\n",
        "\\langle \\Psi |H|\\Psi \\rangle =\\sum_{\\alpha} c_\\alpha \\langle \\Psi | h_\\alpha|\\Psi \\rangle.\n",
        "$$\n",
        "\n",
        "And in the context of our variational wave function:\n",
        "\n",
        "$$\n",
        "\\langle \\psi(\\vec{\\theta}_i) |H|\\psi(\\vec{\\theta}_i) \\rangle =\\sum_{\\alpha} c_\\alpha \\langle \\psi(\\vec{\\theta}_i) | h_\\alpha|\\psi(\\vec{\\theta}_i) \\rangle\n",
        "$$\n",
        "\n",
        "Each of the terms $h_\\alpha$ can be measured $M$ times yielding measurement samples $s_{\\alpha j}$ with $j=1…M$ and returns an expectation value $\\mu_\\alpha$ and a standard deviation $\\sigma_\\alpha$. We can sum these terms and propagate errors through the sum to obtain an overall expectation value $\\mu$ and standard deviation $\\sigma$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\langle \\psi(\\vec{\\theta}_i) |h_\\alpha|\\psi(\\vec{\\theta}_i) \\rangle &\\simeq \\mu _\\alpha \\pm \\frac{\\sigma_\\alpha}{\\sqrt{M}} &\\qquad \\mu_\\alpha &=\\frac{1}{M}\\sum_j s_{\\alpha,j} &\\qquad \\sigma^2_\\alpha &=\\frac{1}{M-1}\\sum_j (s_{\\alpha,j}-\\mu_\\alpha)^2\\\\\n",
        "\n",
        "\\langle \\psi(\\vec{\\theta}_i) |H|\\psi(\\vec{\\theta}_i) \\rangle &\\simeq \\mu  \\pm \\sigma &\\qquad \\mu &= \\sum_\\alpha c_\\alpha \\mu_\\alpha &\\qquad \\sigma^2&=\\sum_\\alpha c^2_\\alpha \\frac{\\sigma^2_\\alpha }{M}\n",
        "\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab02194a-bb0e-4bbc-81e1-c5f8f9582d9b",
      "metadata": {},
      "source": [
        "This requires no large-scale multiplication, nor any process that necessarily scales like $N^2$. Instead it requires multiple measurements on the quantum computer. If you don’t need too many of those, this approach could be efficient. And that’s the quantum part of VQE.\n",
        "\n",
        "But let’s talk about reasons why this might not be efficient. One reason for many measurements is to reduce the statistical uncertainty in your estimates, for very high-precision calculations. Another reason is the number of Pauli strings required to span your entire matrix. Because Pauli matrices (plus the identity: X, Y, Z, and I) span the space of all operators of a given dimension, we are guaranteed that we can write our matrix of interest as a weighted sum of Pauli operators, as we did before.\n",
        "\n",
        "$$\n",
        "H = \\sum_{\\alpha = 1}^T{c_\\alpha h_\\alpha}\n",
        "$$\n",
        "\n",
        "where $h_\\alpha$ is a Pauli string acting on all the qubits describing your system like `IZZX…XIYX`, or several such strings that commute with each other. Recall that Qiskit uses *little endian* notation, in which the $n^\\text{th}$ Pauli operator from the right acts on the $n^\\text{th}$ qubit. So we can measure our operator by measuring a series of Pauli operators.\n",
        "\n",
        "But we cannot measure all those Pauli operators simultaneously. Pauli operators (excluding I) do not commute with each other if they are associated with the same qubit. For example, we can measure `IZIZ` and `ZZXZ` simultaneously, because we can measure I and Z simultaneously for the 3rd qubit, and we can know I and X simultaneously for the 1st qubit. But we cannot measure ZZZZ and ZZZX simultaneously, because Z and X do not commute, and both act on the 0th qubit.\n",
        "\n",
        "So we decompose our matrix $H$ into a sum of Paulis acting on different qubits. Some elements of that sum can be measured all at once; we call this a *group of commuting Paulis*. Depending on how many non-commuting terms there are, we may need many such groups. Call the number of such groups of commuting Pauli strings $N_\\text{GCP}$. If $N_\\text{GCP}$ is small, this could work well. If $H$ has millions of groups, this will not be useful.\n",
        "\n",
        "The processes required for estimation of the expectation value are collected together in the Qiskit Runtime primitive called Estimator. To learn more about Estimator, see the [API reference](/docs/api/qiskit-ibm-runtime/estimator-v2) in IBM Quantum® Documentation. One can simply use Estimator directly, but Estimator returns much more than just the lowest energy eigenvalue. For example, it also returns information on ensemble standard error. Thus, in the context of minimization problems, one often sees Estimator inside a cost function. To learn more about Estimator inputs and outputs see this [guide](/docs/guides/primitive-input-output) on IBM Quantum Documentation.\n",
        "\n",
        "You record the expectation value (or the cost function) for the set of parameters $\\vec{\\theta_i}$ used in your state, and then you update the parameters. Over time, you could use the expectation values or cost-function values you’ve estimated to approximate a gradient of your cost function in the subspace of states sampled by your ansatz. Both gradient-based, and gradient-free classical optimizers exist. Both suffer from potential trainability issues, like multiple local minima, and large regions of parameter space with near-zero gradient, called *barren plateaus*.\n",
        "\n",
        "### 2.3 Factors that determine computational cost\n",
        "\n",
        "VQE will not solve all your toughest quantum chemistry problems. No. But being better at all calculations is not the point. We have shifted what determines the computational cost.\n",
        "\n",
        "We’ve shifted from a process whose complexity depends only on matrix dimension to one that depends on required precision and the number of non-commuting Pauli operators that make up the matrix. The last bit has no analog in classical computing.\n",
        "\n",
        "Based on these dependencies, for sparse matrices, or matrices involving few non-commuting Pauli strings, this process may be useful. This is the case for systems of interacting spins, for example. For dense matrices, it may be less useful."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a687e3-ae98-49f7-9b5a-0a2700ecd278",
      "metadata": {},
      "source": [
        "## 3. Example Hamiltonian\n",
        "\n",
        "Let us put this algorithm into practice using a small Hamiltonian matrix so that we can see what is happening in each step. We will employ the Qiskit patterns framework:\n",
        "\n",
        "-**Step 1**: Map problem to quantum circuits and operators\n",
        "-**Step 2**: Optimize for target hardware\n",
        "-**Step 3**: Execute on target hardware\n",
        "-**Step 4**: Post-process results\n",
        "\n",
        "### 3.1 Step 1: Map the problem to quantum circuits and operators\n",
        "\n",
        "We will use the one defined above from the chemistry context. We start with some general imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb6fb0c-4b8c-4ab3-b632-ed201e99b45f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# General imports\n",
        "import numpy as np\n",
        "\n",
        "# SciPy minimizer routine\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Plotting functions\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a713b494-ab7a-49c6-845c-db728c4635eb",
      "metadata": {},
      "source": [
        "Again, we assume the Hamiltonian of interest is known. We will use an extremely small Hamiltonian here, because other methods discussed in this course will be more efficient at solving larger problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0908f251-58af-49f9-98a8-af98110f6c15",
      "metadata": {},
      "outputs": [],
      "source": [
        "from qiskit.quantum_info import SparsePauliOp\n",
        "import numpy as np\n",
        "\n",
        "hamiltonian = SparsePauliOp.from_list(\n",
        "    [(\"YZ\", 0.3980), (\"ZI\", -0.3980), (\"ZZ\", -0.0113), (\"XX\", 0.1810)]\n",
        ")\n",
        "\n",
        "A = np.array(hamiltonian)\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(A)\n",
        "print(\"The ground state energy is \", min(eigenvalues))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hamiltonian = SparsePauliOp.from_list(\n",
        "    [(\"II\", 0.5), (\"XX\", 0.5), (\"YY\", 0.5), (\"ZZ\", 0.5)]\n",
        ")\n",
        "\n",
        "hamiltonian = SparsePauliOp(\n",
        "    [\n",
        "        \"IIII\",\n",
        "        \"IIIZ\",\n",
        "        \"IZII\",\n",
        "        \"IIZI\",\n",
        "        \"ZIII\",\n",
        "        \"IZIZ\",\n",
        "        \"IIZZ\",\n",
        "        \"ZIIZ\",\n",
        "        \"IZZI\",\n",
        "        \"ZZII\",\n",
        "        \"ZIZI\",\n",
        "        \"YYYY\",\n",
        "        \"XXYY\",\n",
        "        \"YYXX\",\n",
        "        \"XXXX\",\n",
        "    ],\n",
        "    coeffs=[\n",
        "        -0.09820182 + 0.0j,\n",
        "        -0.1740751 + 0.0j,\n",
        "        -0.1740751 + 0.0j,\n",
        "        0.2242933 + 0.0j,\n",
        "        0.2242933 + 0.0j,\n",
        "        0.16891402 + 0.0j,\n",
        "        0.1210099 + 0.0j,\n",
        "        0.16631441 + 0.0j,\n",
        "        0.16631441 + 0.0j,\n",
        "        0.1210099 + 0.0j,\n",
        "        0.17504456 + 0.0j,\n",
        "        0.04530451 + 0.0j,\n",
        "        0.04530451 + 0.0j,\n",
        "        0.04530451 + 0.0j,\n",
        "        0.04530451 + 0.0j,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bf2097-1a9c-48d5-8eff-393e0c9eb2f9",
      "metadata": {},
      "source": [
        "There are many prefabricated ansatz choices in Qiskit. We will use `efficient_su2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84d6380e-8ee8-4340-a416-c07900fe4c5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-defined ansatz circuit and operator class for Hamiltonian\n",
        "from qiskit.circuit.library import efficient_su2\n",
        "\n",
        "# Note that it is more common to place initial 'h' gates outside the ansatz. Here we specifically wanted this layer structure.\n",
        "ansatz = efficient_su2(\n",
        "    hamiltonian.num_qubits, su2_gates=[\"h\", \"rz\", \"y\"], entanglement=\"circular\", reps=1\n",
        ")\n",
        "\n",
        "num_params = ansatz.num_parameters\n",
        "print(\"This circuit has \", num_params, \"parameters\")\n",
        "\n",
        "ansatz.decompose().draw(\"mpl\", style=\"iqp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15394107-6589-4d06-b611-8fa641784b33",
      "metadata": {},
      "source": [
        "Different ansätze will have different entangling structures and different rotation gates. The one shown here uses CNOT gates for entangling, and both Y gates and parametrized RZ gates for rotations. Note the size of this parameter space; it means we must minimize the cost function over 4 variables (the parameters for the RZ gates). This can be scaled up, but not indefinitely. Running a similar problem on 4 qubits, using the default 3 reps for `efficient_su2` yields 16 variational parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12ee2d18-b4db-4b94-ab19-8a0654ca4b2e",
      "metadata": {},
      "source": [
        "### 3.2 Step 2: Optimize for target hardware\n",
        "\n",
        "The ansatz was written using familiar gates, but our circuit must be transpiled to make use of the basis gates that can be implemented on each quantum computer. We select the least busy backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1a1493-e807-44b8-b971-69f098981da2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# runtime imports\n",
        "from qiskit_ibm_runtime import QiskitRuntimeService\n",
        "from qiskit_ibm_runtime import EstimatorV2 as Estimator\n",
        "from qiskit_ibm_runtime import Session\n",
        "from qiskit.transpiler import CouplingMap, generate_preset_pass_manager\n",
        "from qiskit.visualization import plot_histogram\n",
        "from qiskit_aer import AerSimulator\n",
        "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
        "from qiskit_ibm_runtime.fake_provider import FakeManilaV2\n",
        "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84200023-d16f-41d4-9791-c6ef3488da90",
      "metadata": {},
      "source": [
        "We can now transpile our circuit for this hardware and visualize our transpiled ansatz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1cdcf3-7d94-4a1f-b675-9549bf28d956",
      "metadata": {},
      "outputs": [],
      "source": [
        "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
        "\n",
        "fake_manila = FakeManilaV2()\n",
        "pm = generate_preset_pass_manager(backend=fake_manila, optimization_level=3)\n",
        "backend=fake_manila\n",
        "\n",
        "ansatz_isa = pm.run(ansatz)\n",
        "ansatz_isa.draw(output=\"mpl\", idle_wires=False, style=\"iqp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6066c477-2d43-459f-bfc6-650011279587",
      "metadata": {},
      "source": [
        "Note that the gates used have changed, and the qubits in our abstract circuit have been mapped to differently-numbered qubits on the quantum computer. We must map our Hamiltonian identically in order for our results to be meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3543e888-6f84-4f12-88a8-948dec7f3f55",
      "metadata": {},
      "outputs": [],
      "source": [
        "hamiltonian_isa = hamiltonian.apply_layout(layout=ansatz_isa.layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1ae1c5-c26c-42f8-b3af-411970d4d39d",
      "metadata": {},
      "source": [
        "### 3.3 Step 3: Execute on target hardware\n",
        "\n",
        "#### 3.3.1 Reporting out values\n",
        "\n",
        "We define a cost function here that takes as arguments the structures we have built in previous steps: the parameters, the ansatz, and the Hamiltonian. It also uses the estimator which we have not yet defined. We include code to track the history of our cost function, so that we can check convergence behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb1d8999-77aa-4c1a-adae-8974eda9e63b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cost_func(params, ansatz, hamiltonian, estimator):\n",
        "    \"\"\"Return estimate of energy from estimator\n",
        "\n",
        "    Parameters:\n",
        "        params (ndarray): Array of ansatz parameters\n",
        "        ansatz (QuantumCircuit): Parameterized ansatz circuit\n",
        "        hamiltonian (SparsePauliOp): Operator representation of Hamiltonian\n",
        "        estimator (EstimatorV2): Estimator primitive instance\n",
        "        cost_history_dict: Dictionary for storing intermediate results\n",
        "\n",
        "    Returns:\n",
        "        float: Energy estimate\n",
        "    \"\"\"\n",
        "    pub = (ansatz, [hamiltonian], [params])\n",
        "    result = estimator.run(pubs=[pub]).result()\n",
        "    energy = result[0].data.evs[0]\n",
        "\n",
        "    cost_history_dict[\"iters\"] += 1\n",
        "    cost_history_dict[\"prev_vector\"] = params\n",
        "    cost_history_dict[\"cost_history\"].append(energy)\n",
        "    print(f\"Iters. done: {cost_history_dict['iters']} [Current cost: {energy}]\")\n",
        "\n",
        "    return energy\n",
        "\n",
        "\n",
        "cost_history_dict = {\n",
        "    \"prev_vector\": None,\n",
        "    \"iters\": 0,\n",
        "    \"cost_history\": [],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95537d30-ba5d-48ab-bf6a-82edc8f0a348",
      "metadata": {},
      "source": [
        "It is highly advantageous if you can choose initial parameter values based on knowledge of the problem at hand and characteristics of the target state. We will make no assumptions of such knowledge and use random initial values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a5d370-2947-4441-b71b-ffbdeddf52f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "x0 = 2 * np.pi * np.random.random(num_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0d33b8-7f9c-428d-a76d-d832747b8430",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This required 13 min, 20 s QPU time on an Eagle processor, 28 min total time.\n",
        "with Session(backend=backend) as session:\n",
        "    estimator = Estimator(mode=session)\n",
        "    estimator.options.default_shots = 10000\n",
        "\n",
        "    res = minimize(\n",
        "        cost_func,\n",
        "        x0,\n",
        "        args=(ansatz_isa, hamiltonian_isa, estimator),\n",
        "        method=\"cobyla\",\n",
        "        options={\"maxiter\": 50},\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c958e5-f70e-4736-889a-f88a69c50890",
      "metadata": {},
      "source": [
        "### 3.4 Step 4: Post-process results\n",
        "\n",
        "If the procedure terminates correctly, then the values in our dictionary should be equal to the solution vector and total number of function evaluations, respectively. This is easy to verify:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87046c1-bfe9-4bb3-b7fd-1e4da55149fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "cost_history_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a789373a-8d32-4761-ba21-6b2f98a7ae5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "x = np.linspace(0, 10, 50)\n",
        "\n",
        "# Define the constant function\n",
        "constant = min(eigenvalues)\n",
        "y_constant = np.full_like(x, constant)\n",
        "ax.plot(\n",
        "    range(cost_history_dict[\"iters\"]), cost_history_dict[\"cost_history\"], label=\"VQE\"\n",
        ")\n",
        "ax.set_xlabel(\"Iterations\")\n",
        "ax.set_ylabel(\"Cost\")\n",
        "ax.plot(y_constant, label=\"Target\")\n",
        "plt.legend()\n",
        "plt.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be8cb84-ec8c-4c83-880f-b9c296282040",
      "metadata": {},
      "source": [
        "IBM Quantum has other upskilling offerings related to VQE. If you are ready to put VQE into practice, see our tutorial: [Ground-state energy estimation of the Heisenberg chain with VQE](/docs/tutorials/spin-chain-vqe). If you want more information on creating molecular Hamiltonians, see [this lesson](/learning/courses/quantum-chem-with-vqe/hamiltonian-construction) in our course on [Quantum chemistry with VQE](/learning/courses/quantum-chem-with-vqe). If you are interested in a deeper understanding of how variational algorithms like VQE work, we recommend the course [Variational Algorithm Design](/learning/courses/variational-algorithm-design/optimization-loops)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33231f54-a9c1-499c-a4ad-b4404b843909",
      "metadata": {},
      "source": [
        "## 4. The strengths and weaknesses of VQE\n",
        "\n",
        "Some strengths have already been pointed out. They include:\n",
        "\n",
        "*   Suitability to modern hardware: Some quantum algorithms require much lower error rates, approaching large scale fault tolerance. VQE does not; it can be implemented on current quantum computers.\n",
        "*   Shallow circuits: VQE often employs relatively shallow quantum circuits. This makes VQE less susceptible to accumulated gate errors and makes it suitable for many error mitigation techniques. Of course, the circuits are not always shallow; this depends on the ansatz used.\n",
        "*   Versatility: VQE can (in principle) be applied to any problem that can be cast as an eigenvalue/eigenvector problem. There are many caveats that make VQE impractical or disadvantageous for some problems. Some of these are recapped below.\n",
        "\n",
        "Some weaknesses of VQE and problems for which it is impractical have also been described above. These include:\n",
        "\n",
        "*   Heuristic nature: VQE does not guarantee convergence to the correct ground state energy, as its performance depends on the choice of ansatz and optimization methods[\\[1-2\\]](#references). If a poor ansatz is chosen that lacks the requisite entanglement for the desired ground state, no classical optimizer can reach that ground state.\n",
        "*   Potentially numerous parameters: A very expressive ansatz may have so many parameters that the minimization iterations are very time-consuming.\n",
        "*   High measurement overhead: In VQE, an estimator is used to estimate the expectation value of each term in the Hamiltonian. Most Hamiltonians of interest will have terms that cannot be simultaneously estimated. This can make VQE resource-intensive for large systems with complicated Hamiltonians[\\[1\\]](#references).\n",
        "*   Effects of noise: When the classical optimizer is searching for a minimum, noisy calculations can confuse it and steer it away from the true minimum or delay its convergence. One possible solution for this is leveraging state-of-the-art error mitigation and error suppression techniques[\\[2-3\\]](#references) from IBM.\n",
        "*   Barren plateaus: These regions of vanishing gradients[\\[2-3\\]](#references) exist even in the absence of noise, but noise makes them more troublesome since the change in expectation values due to noise could be larger than the change from updating parameters in these barren regions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8086b6a5-daf2-457b-bc7a-84db943b333f",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "\\[2] [https://en.wikipedia.org/wiki/Variational\\_quantum\\_eigensolver](https://en.wikipedia.org/wiki/Variational_quantum_eigensolver)\n",
        "\n",
        "\\[3] [https://journals.aps.org/prapplied/abstract/10.1103/PhysRevApplied.19.024047](https://journals.aps.org/prapplied/abstract/10.1103/PhysRevApplied.19.024047)\n",
        "\n",
        "\\[4] [https://arxiv.org/abs/2111.05176](https://arxiv.org/abs/2111.05176)\n",
        "\n",
        "\\[6] [https://inquanto.quantinuum.com/tutorials/InQ\\_tut\\_fe4n2\\_2.html](https://inquanto.quantinuum.com/tutorials/InQ_tut_fe4n2_2.html)\n",
        "\n",
        "\\[7] [https://www.nature.com/articles/s41467-019-10988-2](https://www.nature.com/articles/s41467-019-10988-2)\n",
        "\n",
        "\\[8] [https://arxiv.org/abs/2210.15438](https://arxiv.org/abs/2210.15438)\n",
        "\n",
        "\\[9] [https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.6.013254](https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.6.013254)\n",
        "\n",
        "\\[10] [https://arxiv.org/html/2403.09624v1](https://arxiv.org/html/2403.09624v1)\n",
        "\n",
        "\\[11] [https://www.nature.com/articles/s42005-023-01312-y](https://www.nature.com/articles/s42005-023-01312-y)\n",
        "\n",
        "\\[13] [https://arxiv.org/abs/1802.00171](https://arxiv.org/abs/1802.00171)\n",
        "\n",
        "\\[14] [https://arxiv.org/abs/2103.08505](https://arxiv.org/abs/2103.08505)\n",
        "\n",
        "\\[15] [https://arxiv.org/html/2501.09702v1](https://arxiv.org/html/2501.09702v1)\n",
        "\n",
        "\\[16] [https://quri-sdk.qunasys.com/docs/examples/quri-algo-vm/qsci/](https://quri-sdk.qunasys.com/docs/examples/quri-algo-vm/qsci/)\n",
        "\n",
        "\\[17] [https://arxiv.org/abs/2412.13839](https://arxiv.org/abs/2412.13839)\n",
        "\n",
        "\\[18] [https://arxiv.org/abs/2302.11320v1](https://arxiv.org/abs/2302.11320v1)\n",
        "\n",
        "\\[19] [https://arxiv.org/pdf/2405.05068v1](https://arxiv.org/pdf/2405.05068v1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b8767d",
      "metadata": {},
      "source": [
        "© IBM Corp., 2017-2025"
      ]
    }
  ],
  "metadata": {
    "description": "This introduction to VQE covers its components, a basic implementation, and discusses what factors determine its efficiency and usefulness.",
    "kernelspec": {
      "display_name": "Qiskit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "title": "Variational Quantum Eigensolver"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
